{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Write a simple CUDA kernel that takes an array of integers and doubles each element.**"
      ],
      "metadata": {
        "id": "3n714FyEdcaX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nxLk8gLmcx1_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "2cf4eaa9-8bbc-4df4-cf20-17c9225c599c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#include <iostream>\\n#include <cuda_runtime.h>\\n\\nusing namespace std;\\n\\n__global__ void add_basic(int n, int *x)\\n{\\n    for (int i = 0; i < n; i++)\\n    {\\n        x[i] *= 2;\\n    }\\n}\\n\\nint main()\\n{\\n    int N = 10;\\n    int *x;\\n\\n    cudaMallocManaged(&x, N * sizeof(int));\\n\\n    for (int i = 0; i < N; i++)\\n    {\\n        x[i] = i + 1;\\n    }\\n\\n    cout << \"Before Doubling = First 2 elements of the array:\" << endl;\\n    for (int i = 0; i < 2; ++i)\\n    {\\n        cout << x[i] << \" \";\\n    }\\n    cout << endl;\\n\\n    add_basic<<<1, 1>>>(N, x);\\n\\n    // Wait for GPU to finish before accessing on host\\n    cudaDeviceSynchronize();\\n\\n    cout << \"After Doubling = First 2 elements of the array:\" << endl;\\n    for (int i = 0; i < 2; ++i)\\n    {\\n        cout << x[i] << \" \";\\n    }\\n    cout << endl;\\n\\n    cudaFree(x);\\n\\n    return 0;\\n}\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "! touch add_basic.cu\n",
        "\"\"\"\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "__global__ void add_basic(int n, int *x)\n",
        "{\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        x[i] *= 2;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int N = 10;\n",
        "    int *x;\n",
        "\n",
        "    cudaMallocManaged(&x, N * sizeof(int));\n",
        "\n",
        "    for (int i = 0; i < N; i++)\n",
        "    {\n",
        "        x[i] = i + 1;\n",
        "    }\n",
        "\n",
        "    cout << \"Before Doubling = First 2 elements of the array:\" << endl;\n",
        "    for (int i = 0; i < 2; ++i)\n",
        "    {\n",
        "        cout << x[i] << \" \";\n",
        "    }\n",
        "    cout << endl;\n",
        "\n",
        "    add_basic<<<1, 1>>>(N, x);\n",
        "\n",
        "    // Wait for GPU to finish before accessing on host\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    cout << \"After Doubling = First 2 elements of the array:\" << endl;\n",
        "    for (int i = 0; i < 2; ++i)\n",
        "    {\n",
        "        cout << x[i] << \" \";\n",
        "    }\n",
        "    cout << endl;\n",
        "\n",
        "    cudaFree(x);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. Write a CUDA kernel to initialize an array of integers with the index value.**"
      ],
      "metadata": {
        "id": "8IFT0vwhdhKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! touch add_basic.cu\n",
        "\"\"\"\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "__global__ void initialize_array(int *array, int size)\n",
        "{\n",
        "    // Calculate the index for the current thread\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (index < size)\n",
        "    {\n",
        "        array[index] = index;\n",
        "    }\n",
        "    // COMPLETE THIS\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    const int array_size = 10;\n",
        "    int *d_array;\n",
        "\n",
        "    // Allocate memory on GPU\n",
        "    cudaMalloc((void **)&d_array, array_size * sizeof(int));\n",
        "\n",
        "    // Launch the CUDA kernel to initialize the array\n",
        "    initialize_array<<<1, array_size>>>(d_array, array_size);\n",
        "\n",
        "    // Copy data from device to host\n",
        "    int h_array[array_size];\n",
        "    cudaMemcpy(h_array, d_array, array_size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print the initialized array\n",
        "    cout << \"Initialized Array:\" << endl;\n",
        "    for (int i = 0; i < array_size; ++i)\n",
        "    {\n",
        "        cout << h_array[i] << \" \";\n",
        "    }\n",
        "    cout << endl;\n",
        "\n",
        "    // Free GPU memory\n",
        "    cudaFree(d_array);\n",
        "\n",
        "    // Wait for GPU to finish before accessing on host\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ozXZzwCsdhcu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "1c7bdc66-420f-482b-a073-58eb31523d43"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#include <iostream>\\n#include <cuda_runtime.h>\\n\\nusing namespace std;\\n\\n__global__ void initialize_array(int *array, int size)\\n{\\n    // Calculate the index for the current thread\\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\\n    if (index < size)\\n    {\\n        array[index] = index;\\n    }\\n    // COMPLETE THIS\\n}\\n\\nint main()\\n{\\n    const int array_size = 10;\\n    int *d_array;\\n\\n    // Allocate memory on GPU\\n    cudaMalloc((void **)&d_array, array_size * sizeof(int));\\n\\n    // Launch the CUDA kernel to initialize the array\\n    initialize_array<<<1, array_size>>>(d_array, array_size);\\n\\n    // Copy data from device to host\\n    int h_array[array_size];\\n    cudaMemcpy(h_array, d_array, array_size * sizeof(int), cudaMemcpyDeviceToHost);\\n\\n    // Print the initialized array\\n    cout << \"Initialized Array:\" << endl;\\n    for (int i = 0; i < array_size; ++i)\\n    {\\n        cout << h_array[i] << \" \";\\n    }\\n    cout << endl;\\n\\n    // Free GPU memory\\n    cudaFree(d_array);\\n\\n    // Wait for GPU to finish before accessing on host\\n    cudaDeviceSynchronize();\\n\\n    return 0;\\n}\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3 [OPTIONAL]. How do you check for and handle errors in CUDA API calls and kernel launches?**"
      ],
      "metadata": {
        "id": "8uewwr30fg-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! touch add_basic_error_handling.cu\n",
        "\"\"\"\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "#define CHECK_CUDA_ERROR(err)                                          \\\n",
        "    {                                                                  \\\n",
        "        if (err != cudaSuccess)                                        \\\n",
        "        {                                                              \\\n",
        "            cerr << \"CUDA Error: \" << cudaGetErrorString(err) << endl; \\\n",
        "            exit(EXIT_FAILURE);                                        \\\n",
        "        }                                                              \\\n",
        "    }\n",
        "\n",
        "__global__ void initialize_array(int *array, int size)\n",
        "{\n",
        "    // Calculate the index for the current thread\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (index < size)\n",
        "    {\n",
        "        array[index] = index;\n",
        "    }\n",
        "    // COMPLETE THIS\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    const int array_size = 10;\n",
        "    int *d_array;\n",
        "\n",
        "    // Allocate memory on GPU\n",
        "    CHECK_CUDA_ERROR(cudaMalloc((void **)&d_array, array_size * sizeof(int)));\n",
        "\n",
        "    // Launch the CUDA kernel to initialize the array\n",
        "    initialize_array<<<1, array_size>>>(d_array, array_size);\n",
        "    CHECK_CUDA_ERROR(cudaGetLastError());\n",
        "\n",
        "    // Copy data from device to host\n",
        "    int h_array[array_size];\n",
        "    CHECK_CUDA_ERROR(cudaMemcpy(h_array, d_array, array_size * sizeof(int), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // Print the initialized array\n",
        "    cout << \"Initialized Array:\" << endl;\n",
        "    for (int i = 0; i < array_size; ++i)\n",
        "    {\n",
        "        cout << h_array[i] << \" \";\n",
        "    }\n",
        "    cout << endl;\n",
        "\n",
        "    // Free GPU memory\n",
        "    CHECK_CUDA_ERROR(cudaFree(d_array));\n",
        "\n",
        "    // Wait for GPU to finish before accessing on host\n",
        "    CHECK_CUDA_ERROR(cudaDeviceSynchronize());\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RYdRBoA5fg1X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "8a55c29a-d166-4833-8d67-f0e1c0ccfdaa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#include <iostream>\\n#include <cuda_runtime.h>\\n\\nusing namespace std;\\n\\n#define CHECK_CUDA_ERROR(err)                                                        {                                                                                    if (err != cudaSuccess)                                                          {                                                                                    cerr << \"CUDA Error: \" << cudaGetErrorString(err) << endl;             exit(EXIT_FAILURE);                                                          }                                                                            }\\n\\n__global__ void initialize_array(int *array, int size)\\n{\\n    // Calculate the index for the current thread\\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\\n    if (index < size)\\n    {\\n        array[index] = index;\\n    }\\n    // COMPLETE THIS\\n}\\n\\nint main()\\n{\\n    const int array_size = 10;\\n    int *d_array;\\n\\n    // Allocate memory on GPU\\n    CHECK_CUDA_ERROR(cudaMalloc((void **)&d_array, array_size * sizeof(int)));\\n\\n    // Launch the CUDA kernel to initialize the array\\n    initialize_array<<<1, array_size>>>(d_array, array_size);\\n    CHECK_CUDA_ERROR(cudaGetLastError());\\n\\n    // Copy data from device to host\\n    int h_array[array_size];\\n    CHECK_CUDA_ERROR(cudaMemcpy(h_array, d_array, array_size * sizeof(int), cudaMemcpyDeviceToHost));\\n\\n    // Print the initialized array\\n    cout << \"Initialized Array:\" << endl;\\n    for (int i = 0; i < array_size; ++i)\\n    {\\n        cout << h_array[i] << \" \";\\n    }\\n    cout << endl;\\n\\n    // Free GPU memory\\n    CHECK_CUDA_ERROR(cudaFree(d_array));\\n\\n    // Wait for GPU to finish before accessing on host\\n    CHECK_CUDA_ERROR(cudaDeviceSynchronize());\\n\\n    return 0;\\n}\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}